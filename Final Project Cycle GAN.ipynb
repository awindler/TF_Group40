{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "\n",
    "This is a raw version containing copied passages from the 8th homework implementing a generative adversarial network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download the MNIST and the SVHN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read in the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing mnist\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "data = read_idx('../MNIST/train-images.idx3-ubyte')\n",
    "training_labels = read_idx('../MNIST/train-labels.idx1-ubyte')\n",
    "\n",
    "\n",
    "### not done yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Helper functions for leyers\n",
    "\n",
    "Following cells contain helper functions to create different leyers of our model. (JUST COPIED FROM OLD HOMEWORK (8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# not sure which we will use and maybe we have to change some\n",
    "\n",
    "def feed_forward_layer(x, hidden_n, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    weights = tf.get_variable(\"weights\", [x.shape[1], hidden_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [hidden_n], tf.float32, tf.zeros_initializer())\n",
    "   \n",
    "    drive = tf.matmul(x, weights) + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0])\n",
    "   \n",
    "    if activation_fn == 'linear':\n",
    "        return drive\n",
    "    else:\n",
    "        return activation_fn(drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, kernels_n, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, x.shape[-1], kernels_n], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [kernels_n], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d(x, kernels, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_conv_layer(x, target_shape, kernel_size, stride_size, activation_fn, normalize):\n",
    "    initializer = tf.random_normal_initializer(stddev=0.02)\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, target_shape[-1], x.shape[-1]], tf.float32, initializer)\n",
    "    biases = tf.get_variable(\"biases\", [target_shape[-1]], tf.float32, tf.zeros_initializer())\n",
    "\n",
    "    drive = tf.nn.conv2d_transpose(x, kernels, target_shape, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "    if normalize:\n",
    "        drive = batch_norm(drive, [0,1,2])\n",
    "    \n",
    "    return activation_fn(drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, axes):\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    offset_initializer = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [x.shape[-1]], tf.float32, offset_initializer)\n",
    "    scale_initializer = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [x.shape[-1]], tf.float32, scale_initializer)\n",
    "    return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generator\n",
    "The generator creates new images.\n",
    "#### 2.2.1 Input to generator\n",
    "As an input for our generator xy vectors with xy dimensions are used. Because we have o feed in these vectors we need to define a placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can create our graph, we need to reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the placeholder for the generator input\n",
    "generator_input = tf.placeholder(tf.float32, shape=(32, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Generator Layer 1\n",
    "The first layer of the generator is a feed forward layer with XY hidden neurons, XY activation function and batch normalization(?). After the feed forward step the output is reshaped to XY feature maps of size XxY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('generator_layer_1', reuse=tf.AUTO_REUSE):\n",
    "    gen_output_unshaped = feed_forward_layer(x=generator_input, \n",
    "                                             hidden_n=1024, \n",
    "                                             activation_fn=tf.nn.relu, \n",
    "                                             normalize=True)\n",
    "    gen_output = tf.reshape(gen_output_unshaped, [32, 4, 4, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####followingly define different leyers for the generater, e.g.\n",
    "\n",
    "with tf.variable_scope('generator_layer_2', reuse=tf.AUTO_REUSE):\n",
    "    gen_output = back_conv_layer(x=gen_output, \n",
    "                                  target_shape=[32, 7, 7, 32], \n",
    "                                  kernel_size=5, \n",
    "                                  stride_size=2, \n",
    "                                  activation_fn=tf.nn.relu, \n",
    "                                  normalize=True)\n",
    "    \n",
    "with tf.variable_scope('gengenerator_layer_3', reuse=tf.AUTO_REUSE):\n",
    "    gen_output = back_conv_layer(x=gen_output, \n",
    "                                  target_shape=[32, 14, 14, 16], \n",
    "                                  kernel_size=5, \n",
    "                                  stride_size=2, \n",
    "                                  activation_fn=tf.nn.relu, \n",
    "                                  normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Discriminator\n",
    "The discriminator wants to find out which images are original and which are generated.\n",
    "#### 2.3.1 Input to Discriminator\n",
    "The input for the discriminator is the XY previously generated images and XY original images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the MNIST training images\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=4, reshuffle_each_iteration=True)\n",
    "\n",
    "# Define the batch\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "# Create the iterator and get the next batch\n",
    "iterator = tf.data.Iterator.from_structure(dataset.output_types,\n",
    "                                           dataset.output_shapes)\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# initializer for MNIST dataset\n",
    "init_op = iterator.make_initializer(dataset)\n",
    "\n",
    "MNIST_images = tf.cast(next_batch, tf.float32)\n",
    "\n",
    "# Concatinate the original MNIST training images and the generated images\n",
    "discriminator_input = tf.concat([gen_output, tf.expand_dims(MNIST_images, axis=-1)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Discriminator Layer 1 & 2 & 3\n",
    "The first three layers of the discriminator are normal convolutional layers with respectively 8, 16 and 32 feature maps. All use leaky ReLU activation function and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator_layer_1', reuse=tf.AUTO_REUSE):\n",
    "    disc_output = conv_layer(x=discriminator_input, \n",
    "                              kernels_n=8, \n",
    "                              kernel_size=5, \n",
    "                              stride_size=2, \n",
    "                              activation_fn=tf.nn.leaky_relu, \n",
    "                              normalize=True)\n",
    "\n",
    "with tf.variable_scope('discriminator_layer_2', reuse=tf.AUTO_REUSE):\n",
    "    disc_output = conv_layer(x=disc_output, \n",
    "                              kernels_n=16, \n",
    "                              kernel_size=5, \n",
    "                              stride_size=2, \n",
    "                              activation_fn=tf.nn.leaky_relu, \n",
    "                              normalize=True)\n",
    "    \n",
    "with tf.variable_scope('discriminator_layer_3', reuse=tf.AUTO_REUSE):\n",
    "    disc_output = conv_layer(x=disc_output, \n",
    "                              kernels_n=32, \n",
    "                              kernel_size=5, \n",
    "                              stride_size=2, \n",
    "                              activation_fn=tf.nn.leaky_relu, \n",
    "                              normalize=True)\n",
    "    \n",
    "    \n",
    "    ####in the hw a fourth leyer was implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training\n",
    "We want to train the generator and the discriminator seperatly.\n",
    "#### 2.4.1 Training the Generator\n",
    "Because we want to fool the discriminator with our generated images, we need 32 ones as labels. We calculate the loss by taking the mean of the sigmoid cross entropy between the labels (32 ones) and the first half of the discriminators output (the logits that correspond to the generated images). Then we take an Adam optimizer to minimize the loss and only change the variables of our generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a list with 32 ones as labels\n",
    "generator_training_labels = tf.ones([32, 1])\n",
    "\n",
    "# Take the first 32 outputs of the discriminator\n",
    "generator_training_outputs = tf.slice(disc_output, [0, 0], [32, 1])\n",
    "\n",
    "# Calculate the loss\n",
    "generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=generator_training_labels, logits=generator_training_outputs))\n",
    "\n",
    "# We just want to train the generator \n",
    "trainable_variables = tf.trainable_variables()\n",
    "generator_variables = [var for var in trainable_variables if 'generator' in var.name]\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=0.0004, beta1=0.5)\n",
    "training_step_generator = generator_optimizer.minimize(generator_loss, var_list=generator_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Training the Discriminator\n",
    "For the training of the discriminator we need 64 labels. 32 ones for the generated images and 32 ones for the original images. Then we calculate the loss and optimize all variables of the discriminator with an Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a list with 32 zeros and a list with 32 ones as labels\n",
    "discriminator_training_labels = tf.concat([tf.zeros([32, 1]), tf.ones([32,1])], axis=0)\n",
    "\n",
    "# Calculate the loss\n",
    "discriminator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=discriminator_training_labels, logits=disc_output))\n",
    "\n",
    "# We just want to train the discriminator\n",
    "trainable_variables = tf.trainable_variables()\n",
    "discriminator_variables = [var for var in trainable_variables if 'discriminator' in var.name]\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=0.0004, beta1=0.5)\n",
    "training_step_discriminator = discriminator_optimizer.minimize(discriminator_loss, var_list=discriminator_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training\n",
    "### 3.1 Monitoring\n",
    "To visualize the losses and the generated images we use TensorBoard. Thus, we create scalar summaries for the losses and an image summary for the generated images. Furthermore, we define two filewriter for the training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add summaries for monitoring the loss\n",
    "generator_loss_summary = tf.summary.scalar('generator-loss', generator_loss)\n",
    "discriminator_loss_summary = tf.summary.scalar('discriminator-loss', discriminator_loss)\n",
    "\n",
    "# Merge loss summaries\n",
    "loss_summary = tf.summary.merge([generator_loss_summary, discriminator_loss_summary])\n",
    "\n",
    "# Add summary for visualizing the generated images\n",
    "generated_images_summary = tf.summary.image('generated-images', gen_output, max_outputs=32)\n",
    "\n",
    "# Define the filewriter for storing the values\n",
    "train_writer = tf.summary.FileWriter(logdir='./summaries/train/')\n",
    "validation_writer = tf.summary.FileWriter(logdir='./summaries/validation/', flush_secs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training and validation\n",
    "We train the model for 2 epochs. In each step we first sample 32 random vectors with 50 dimensions from an uniform distribution. This vectors are than feed in the generator during the training. We then run both training steps (for the generator andd the discriminator) and read out and save the loss summaries. After each 100th step we validate the model. Thus, we feed in the vector sampled at the beginning for the validation and read out the summaries for the generated images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training epoches\n",
    "epochs = 2\n",
    "\n",
    "# start the training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initialize the variables for the session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # initialize step counter for saving the summaries\n",
    "    global_step = 1\n",
    "    \n",
    "    # create the random vector for the validation\n",
    "    validation_input = np.random.uniform(-1, 1, [32, 50])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # load the training data into the iterator\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # create the random vector for the generator\n",
    "                random_input = np.random.uniform(-1, 1, [32, 50])\n",
    "                \n",
    "                # train the generator and discriminator andd read out the loss summaries for the generator and discriminator\n",
    "                # unfortunately, we cannot use the merged summary because this results in the a error we were not able to solve\n",
    "                # error: Fetch argument b'\\n\\x10\\n\\tgenerator-loss\\x15\\xe8TV?\\n\\x14\\n\\rdiscriminator-loss\\x15\\xc2\\xd5/?' cannot be interpreted as a Tensor. ('utf-8' codec can't decode byte 0xe8 in position 19: invalid continuation byte)\n",
    "                # _, _, gen_loss_summary, disc_loss_summary  = sess.run((training_step_generator, training_step_discriminator, generator_loss_summary, discriminator_loss_summary), \n",
    "                #                           feed_dict={generator_input: random_input})\n",
    "                _, _, loss_summaries  = sess.run((training_step_generator, training_step_discriminator, loss_summary), \n",
    "                                          feed_dict={generator_input: random_input})\n",
    "                # write the loss summaries\n",
    "                # train_writer.add_summary(gen_loss_summary, global_step)\n",
    "                # train_writer.add_summary(disc_loss_summary, global_step)\n",
    "                train_writer.add_summary(loss_summaries, global_step)\n",
    "                \n",
    "                # read out the generated images\n",
    "                if (global_step % 100 == 0):\n",
    "                    image_summary = sess.run((generated_images_summary), feed_dict={generator_input: validation_input})\n",
    "                    validation_writer.add_summary(image_summary, global_step)\n",
    "                \n",
    "                # update the step counter\n",
    "                global_step += 1\n",
    "                \n",
    "            # breakout of the loop if we looked at all batches and finished our training\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
